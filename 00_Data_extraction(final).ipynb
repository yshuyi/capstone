{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c150d5-b6e9-4d9e-bc81-ace34a3991d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3727471d-d8e1-4471-a59e-25d629d5822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping reddit posts via api.pushshift. scrapping both submissions and comments. they have different pushshift urls.\n",
    "submission_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "comment_url = 'https://api.pushshift.io/reddit/search/comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8024e82-1e82-4291-ada1-e4526cc20199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_scrapper(url, subreddit, epoch, n_iters, pickle_file):\n",
    "    url = url\n",
    "    params = {'subreddit': subreddit,\n",
    "              'size': 100, #100 is pushshifts max\n",
    "              'before': 1646063999, #1646063999 is 28 Feb 2022. this can be updated or commented out\n",
    "              'after': epoch # \n",
    "             } \n",
    "    request_data = []\n",
    "\n",
    "    for i in tqdm(range(n_iters)):\n",
    "        if len(request_data) > 0:\n",
    "            params['after'] = request_data[-1]['created_utc']\n",
    "\n",
    "        request_data_add = requests.get(url, params).json()['data']\n",
    "        request_data += request_data_add\n",
    "        time.sleep(3)\n",
    "\n",
    "    return pd.DataFrame(request_data).to_pickle(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b14dde9-1b28-4715-9880-252ed7d30798",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [03:59<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 186 ms, total: 1.66 s\n",
      "Wall time: 3min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reddit_scrapper function requires (url) either submission or comment pushshift url, (epoch) epoch time to start scrapping from, (n_iters) the iterations of how many 100 posts to scrap, (pickle_file) the pickle file name to save data\n",
    "reddit_scrapper(submission_url, 'sephora', 1645142400, 50, 'datasets/xxx.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8958631-61d2-4ebc-bb3c-8311d4be3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after scrapping, read the pickle file and convert pickle in csv. check for duplicates before saving over. \n",
    "df = pd.read_pickle('./datasets/xxx.pkl')\n",
    "print(df.shape)\n",
    "df.tail()\n",
    "\n",
    "pd.DataFrame(df).to_csv('datasets/xxx.csv', index=False)\n",
    "df = pd.read_csv('./datasets/', low_memory=False)\n",
    "\n",
    "duplicateRowsDF = df[df.duplicated()]\n",
    "duplicateRowsDF\n",
    "\n",
    "df.drop_duplicates(subset=None, keep= 'first', inplace=True)\n",
    "df.shape\n",
    "\n",
    "pd.DataFrame(df).to_csv('datasets/xxx.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
